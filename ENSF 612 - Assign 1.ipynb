{"cells":[{"cell_type":"markdown","source":["## ENSF 612 - Assignment 1\n### Author: Steven Duong (30022492)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7cd97c31-0d86-430c-b20a-6995cdc7d8ba","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Question 1\n\n### Original Source Code"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1bbc2568-19fb-43a1-8d14-777c2255269d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from collections import defaultdict\nfilteredKV1 = readFile(filename1)\nfilteredKV2 = readFile(filename2)\n\n# The following pyspark method is called to read two files, do word counts on each file, and to\n# return the word counts\n\ndef readFile(filename):\n    infile = sc.textFile(filename) # here assume that sc is SparkContext\n    counts = infile.flatMap(lambda line: line.split(\" \")).map(lambda word:\n    (word, 1)).collect()\n    return doFilter(counts1)\n\ndef doFilter(count):\n    key_val = defaultdict(int)\n    for item in counts:\n    key = item[0]\n    val = item[1]\n    key_val[key] += int(val)\n    filtered_key_val = dict()\n    \n    for k, v in key_val.items():\n        if v >= 100:\n            filtered_key_val[k] = v\n            \n    return filtered_key_val"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0ff220e-9c6a-4591-bb18-c25af540a52f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Issues\n\n1. in the readFile function, doFilter takes the incorrect argument of 'counts1', causing a runtime error. In order to fix this, we would simply pass the argument 'counts' into the doFilter function call.\n\n2. The use of the collect() method: This method collects all the data from the RDD and stores it into memory (driver node), which is usually not a good idea for large datasets. In this case, collecting 2 TB of data into the memory for each file of an 8 GB machine would most likely cause a memory overflow. To solve this error, we can replace the collect() method with the reduceByKey method. This would eliminate the requirement to store temporary results in memory and prevent it from exceeding its capacity.\n\n3. The doFilter function uses for loops and python dictionaries to compute the word count. This function is not efficient because it collects all the data into the driver node's memory and performs the word count in memory. For large datasets, the memory on the driver node may not be sufficient to hold all the data (8 GB of RAM in this case), leading to a crash on a personal machine. A more efficient solution would be to perform the word count in a distributed fashion using the Spark operations of reduceByKey and filter, which allows data to be processed in parallel within a cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bbcd02c-a70c-4b8a-acfd-7c1ee11e46ab","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Updated Code"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66e6631f-323d-49a9-8803-d21c7e0f2e7b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from collections import defaultdict\nfilteredKV1 = readFile(filename1)\nfilteredKV2 = readFile(filename2)\n\n# Replaced the collect() method with the reduceByKey method.\ndef readFile(filename):\n    infile = sc.textFile(filename) # reads the input file as SparkContext and creates an RDD.\n    counts = infile.flatMap(lambda line: line.split(\" \")).map(lambda word:\n    (word, 1)).reduceByKey(lambda a, b: a + b) # aggregates the word-value RDD pairs as word-count RDD pairs. KVP: (word, word count).\n    return doFilter(counts) # pass reduced word-count RDD as an argument.\n\n# Implemented the filter Spark operation to allow for parallel data processing\n# using cluster computing. This method filters the RDD 'counts' to keep only\n# words with at least 100 counts.\ndef doFilter(counts):\n    filtered = counts.filter(lambda x: x[1] >= 100) # x is a tuple, and we compare if: word count >= 100.\n    return filtered # return filtered word-count RDD containing only words with a count of greater or equal to 100."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"691c6b7f-1dea-4adc-8d42-edc47c2529e5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Question 2\n\n### PySpark Programming"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2a91cc3-6cb7-44ba-91af-8bf27ec52187","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Creating a Spark session.\nspark = SparkSession.builder.appName(\"StackOverflowAnalysis\").getOrCreate()\n\n# Function to load the CSV files.\ndef load_csv(filename):\n    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\", \"true\").option('escape', \"\\\"\").load(filename)\n    return df\n\n# Function to show the total ViewCount per file.\ndef total_view_count(df):\n    total_vc = df.agg({\"ViewCount\": \"sum\"}).collect()\n    return total_vc[0][0]\n\n# Function to show the total sample of ViewCounts per file.\ndef view_count_samples(df):\n    return df.count()\n\n# Loading the three CSV files.\ndf_spark = load_csv(\"/FileStore/tables/SO_Spark.csv\")\ndf_ml = load_csv(\"/FileStore/tables/SO_ML.csv\")\ndf_security = load_csv(\"/FileStore/tables/SO_Security.csv\")\n\n# Creating a pyspark dataframe for the total view count sample size.\nlist_of_view_count_samples = [(\"SO-Spark\", view_count_samples(df_spark)), (\"SO-ML\", view_count_samples(df_ml)), (\"SO-Security\", view_count_samples(df_security))]\ndf = spark.createDataFrame(list_of_view_count_samples, ['File Name', 'ViewCount Sample Size'])\ndf.show()\n\n# Creating a pyspark dataframe for the total view count (sum).\nlist_of_view_counts = [(\"SO-Spark\", total_view_count(df_spark)), (\"SO-ML\", total_view_count(df_ml)), (\"SO-Security\", total_view_count(df_security))]\ndf = spark.createDataFrame(list_of_view_counts, ['File Name', 'Total ViewCount (Sum)'])\ndf.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5784340-708e-4dbe-8764-1f09e5a06161","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------+---------------------+\n|  File Name|ViewCount Sample Size|\n+-----------+---------------------+\n|   SO-Spark|                50000|\n|      SO-ML|                50000|\n|SO-Security|                50000|\n+-----------+---------------------+\n\n+-----------+---------------------+\n|  File Name|Total ViewCount (Sum)|\n+-----------+---------------------+\n|   SO-Spark|         1.40958653E8|\n|      SO-ML|         1.02120351E8|\n|SO-Security|         1.63597256E8|\n+-----------+---------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+---------------------+\n|  File Name|ViewCount Sample Size|\n+-----------+---------------------+\n|   SO-Spark|                50000|\n|      SO-ML|                50000|\n|SO-Security|                50000|\n+-----------+---------------------+\n\n+-----------+---------------------+\n|  File Name|Total ViewCount (Sum)|\n+-----------+---------------------+\n|   SO-Spark|         1.40958653E8|\n|      SO-ML|         1.02120351E8|\n|SO-Security|         1.63597256E8|\n+-----------+---------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af3cdcce-7997-40ea-9cd5-82477e8b00e7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ENSF 612 - Assign 1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":244808459795515}},"nbformat":4,"nbformat_minor":0}
